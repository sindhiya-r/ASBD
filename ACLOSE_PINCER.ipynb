{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title 1. ACLOSE algorithm sample transactions - closed frequent patterns\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Sample transactional dataset\n",
        "transactions = {\n",
        "    \"T100\": [\"I1\", \"I2\", \"I5\"],\n",
        "    \"T200\": [\"I2\", \"I4\"],\n",
        "    \"T300\": [\"I2\", \"I3\"],\n",
        "    \"T400\": [\"I1\", \"I2\", \"I4\"],\n",
        "    \"T500\": [\"I1\", \"I3\"],\n",
        "    \"T600\": [\"I2\", \"I3\"],\n",
        "    \"T700\": [\"I1\", \"I3\"],\n",
        "    \"T800\": [\"I1\", \"I2\", \"I3\", \"I5\"],\n",
        "    \"T900\": [\"I1\", \"I2\", \"I3\"]\n",
        "}\n",
        "\n",
        "min_support = 0.1\n",
        "min_count = int(min_support * len(transactions))\n",
        "\n",
        "# Generate candidate itemsets of size 1\n",
        "def get_candidate_1_itemsets(transactions):\n",
        "    item_counts = defaultdict(int)\n",
        "    for transaction in transactions.values():\n",
        "        for item in transaction:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "    return {itemset for itemset, count in item_counts.items() if count >= min_count}\n",
        "\n",
        "# Generate candidate itemsets of size k from frequent (k-1)-itemsets\n",
        "def generate_candidates(prev_frequent_itemsets, k):\n",
        "    candidates = set()\n",
        "    prev_itemsets = list(prev_frequent_itemsets)\n",
        "    for i in range(len(prev_itemsets)):\n",
        "        for j in range(i + 1, len(prev_itemsets)):\n",
        "            candidate = prev_itemsets[i] | prev_itemsets[j]\n",
        "            if len(candidate) == k:\n",
        "                subsets = list(combinations(candidate, k - 1))\n",
        "                if all(frozenset(subset) in prev_frequent_itemsets for subset in subsets):\n",
        "                    candidates.add(candidate)\n",
        "    return candidates\n",
        "\n",
        "# Calculate support count for each candidate itemset\n",
        "def calculate_support(transactions, candidates):\n",
        "    support_counts = defaultdict(int)\n",
        "    for transaction in transactions.values():\n",
        "        transaction_set = frozenset(transaction)\n",
        "        for candidate in candidates:\n",
        "            if candidate.issubset(transaction_set):\n",
        "                support_counts[candidate] += 1\n",
        "    return {itemset: count for itemset, count in support_counts.items() if count >= min_count}\n",
        "\n",
        "# ALCose algorithm to find closed frequent itemsets\n",
        "def alcose_algorithm(transactions, min_support):\n",
        "    k = 1\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    # Initial pass for 1-itemsets\n",
        "    current_frequent_itemsets = get_candidate_1_itemsets(transactions)\n",
        "\n",
        "    while current_frequent_itemsets:\n",
        "        # Add frequent itemsets of size k\n",
        "        frequent_itemsets.extend(current_frequent_itemsets)\n",
        "\n",
        "        # Generate candidate itemsets of size (k+1)\n",
        "        candidates = generate_candidates(current_frequent_itemsets, k + 1)\n",
        "        current_frequent_itemsets = calculate_support(transactions, candidates)\n",
        "\n",
        "        # Filter closed itemsets\n",
        "        closed_itemsets = {}\n",
        "        for itemset, count in current_frequent_itemsets.items():\n",
        "            is_closed = all(count > support for other_itemset, support in current_frequent_itemsets.items()\n",
        "                            if itemset != other_itemset and itemset.issubset(other_itemset))\n",
        "            if is_closed:\n",
        "                closed_itemsets[itemset] = count\n",
        "\n",
        "        current_frequent_itemsets = set(closed_itemsets.keys())\n",
        "        k += 1\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Run ALCose algorithm\n",
        "frequent_closed_itemsets = alcose_algorithm(transactions, min_support)\n",
        "\n",
        "# Output the results\n",
        "print(\"Frequent Closed Itemsets with min_support =\", min_support)\n",
        "for itemset in frequent_closed_itemsets:\n",
        "    print(set(itemset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9ojPRepEe_Jn",
        "outputId": "259f2353-c5a4-4703-c7e2-bc95293f902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Closed Itemsets with min_support = 0.1\n",
            "{'I4'}\n",
            "{'I2'}\n",
            "{'I1'}\n",
            "{'I3'}\n",
            "{'I5'}\n",
            "{'I3', 'I1'}\n",
            "{'I2', 'I4'}\n",
            "{'I1', 'I2'}\n",
            "{'I1', 'I4'}\n",
            "{'I3', 'I5'}\n",
            "{'I2', 'I5'}\n",
            "{'I1', 'I5'}\n",
            "{'I3', 'I2'}\n",
            "{'I1', 'I2', 'I4'}\n",
            "{'I3', 'I2', 'I1'}\n",
            "{'I1', 'I2', 'I5'}\n",
            "{'I3', 'I1', 'I5'}\n",
            "{'I3', 'I2', 'I5'}\n",
            "{'I3', 'I1', 'I2', 'I5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "heoGjoN1nWnK",
        "outputId": "e07d2f38-8e8f-4322-ae31-de7864075491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closed Frequent Patterns and their Supports:\n",
            "Pattern: {85}, Support: 8124\n",
            "Pattern: {34, 85}, Support: 7914\n",
            "Pattern: {36, 85}, Support: 6812\n",
            "Pattern: {85, 39}, Support: 5612\n",
            "Pattern: {59, 85}, Support: 5176\n",
            "Pattern: {85, 63}, Support: 4936\n",
            "Pattern: {85, 86}, Support: 7924\n",
            "Pattern: {90, 85}, Support: 7488\n",
            "Pattern: {34, 85, 86}, Support: 7906\n",
            "Pattern: {34, 90, 85}, Support: 7296\n",
            "Pattern: {36, 85, 86}, Support: 6620\n",
            "Pattern: {90, 36, 85}, Support: 6464\n",
            "Pattern: {85, 86, 39}, Support: 5420\n",
            "Pattern: {90, 85, 39}, Support: 4976\n",
            "Pattern: {34, 36, 85, 86}, Support: 6602\n",
            "Pattern: {34, 85, 86, 39}, Support: 5402\n",
            "Pattern: {34, 59, 85, 86}, Support: 4984\n",
            "Pattern: {34, 90, 85, 86}, Support: 7288\n",
            "Pattern: {34, 36, 85, 86, 90}, Support: 6272\n"
          ]
        }
      ],
      "source": [
        "# @title 1. ACLOSE algorithm on mushroom dataset - closed frequent patterns\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Load the mushroom dataset\n",
        "def load_data(filename):\n",
        "    dataset = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            transaction = frozenset(map(int, line.strip().split()))\n",
        "            dataset.append(transaction)\n",
        "    return dataset\n",
        "\n",
        "# Calculate support for each itemset\n",
        "def calculate_support(dataset, itemset):\n",
        "    return sum(1 for transaction in dataset if itemset.issubset(transaction))\n",
        "\n",
        "# Find all frequent itemsets\n",
        "def find_frequent_itemsets(dataset, min_support):\n",
        "    items = {item for transaction in dataset for item in transaction}\n",
        "    num_transactions = len(dataset)\n",
        "    min_support_count = int(num_transactions * min_support)  # Convert to absolute count\n",
        "    L = []  # List to store all levels of frequent itemsets\n",
        "    L1 = {frozenset([item]): calculate_support(dataset, frozenset([item])) for item in items}\n",
        "    L1 = {k: v for k, v in L1.items() if v >= min_support_count}  # Filter by min_support count\n",
        "    L.append(L1)\n",
        "\n",
        "    k = 2\n",
        "    while L[k - 2]:\n",
        "        Ck = {}\n",
        "        frequent_itemsets_k_1 = list(L[k - 2].keys())\n",
        "        for i in range(len(frequent_itemsets_k_1)):\n",
        "            for j in range(i + 1, len(frequent_itemsets_k_1)):\n",
        "                candidate = frequent_itemsets_k_1[i] | frequent_itemsets_k_1[j]\n",
        "                if len(candidate) == k and all((candidate - frozenset([item]) in frequent_itemsets_k_1) for item in candidate):\n",
        "                    support = calculate_support(dataset, candidate)\n",
        "                    if support >= min_support_count:\n",
        "                        Ck[candidate] = support\n",
        "        L.append(Ck)\n",
        "        k += 1\n",
        "\n",
        "    return L\n",
        "\n",
        "# Extract closed itemsets\n",
        "def extract_closed_itemsets(L):\n",
        "    closed_itemsets = {}\n",
        "    for level in L:\n",
        "        for itemset, support in level.items():\n",
        "            is_closed = True\n",
        "            for higher_level in L:\n",
        "                for superset, superset_support in higher_level.items():\n",
        "                    if itemset < superset and support == superset_support:\n",
        "                        is_closed = False\n",
        "                        break\n",
        "                if not is_closed:\n",
        "                    break\n",
        "            if is_closed:\n",
        "                closed_itemsets[itemset] = support\n",
        "    return closed_itemsets\n",
        "\n",
        "# Main function to run A-Close algorithm\n",
        "def aclose_algorithm(filename, min_support):\n",
        "    dataset = load_data(filename)\n",
        "    frequent_itemsets = find_frequent_itemsets(dataset, min_support)\n",
        "    closed_itemsets = extract_closed_itemsets(frequent_itemsets)\n",
        "    return closed_itemsets\n",
        "\n",
        "# Set parameters\n",
        "filename = '/content/drive/MyDrive/mushroom_dataset/mushroom.dat'\n",
        "min_support = 0.6\n",
        "# Run the algorithm and display results\n",
        "closed_patterns = aclose_algorithm(filename, min_support)\n",
        "print(\"Closed Frequent Patterns and their Supports:\")\n",
        "for pattern, support in closed_patterns.items():\n",
        "    print(f\"Pattern: {set(pattern)}, Support: {support}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Pincer Search on sample transaction - maximal frequent patterns\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Sample transactional dataset\n",
        "transactions = {\n",
        "    \"T100\": [\"I1\", \"I2\", \"I5\"],\n",
        "    \"T200\": [\"I2\", \"I4\"],\n",
        "    \"T300\": [\"I2\", \"I3\"],\n",
        "    \"T400\": [\"I1\", \"I2\", \"I4\"],\n",
        "    \"T500\": [\"I1\", \"I3\"],\n",
        "    \"T600\": [\"I2\", \"I3\"],\n",
        "    \"T700\": [\"I1\", \"I3\"],\n",
        "    \"T800\": [\"I1\", \"I2\", \"I3\", \"I5\"],\n",
        "    \"T900\": [\"I1\", \"I2\", \"I3\"]\n",
        "}\n",
        "\n",
        "min_support = 0.1\n",
        "min_count = int(min_support * len(transactions))\n",
        "\n",
        "# Calculate support count for an itemset\n",
        "def calculate_support(transactions, itemset):\n",
        "    count = 0\n",
        "    for transaction in transactions.values():\n",
        "        if itemset.issubset(transaction):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# Generate candidate itemsets of size k\n",
        "def generate_candidates(frequent_itemsets, k):\n",
        "    candidates = set()\n",
        "    itemsets_list = list(frequent_itemsets)\n",
        "    for i in range(len(itemsets_list)):\n",
        "        for j in range(i + 1, len(itemsets_list)):\n",
        "            candidate = itemsets_list[i] | itemsets_list[j]\n",
        "            if len(candidate) == k:\n",
        "                subsets = list(combinations(candidate, k - 1))\n",
        "                if all(frozenset(subset) in frequent_itemsets for subset in subsets):\n",
        "                    candidates.add(candidate)\n",
        "    return candidates\n",
        "\n",
        "# Pincer-Search Algorithm\n",
        "def pincer_search(transactions, min_count):\n",
        "    max_frequent_patterns = []  # Store maximal frequent itemsets\n",
        "    frequent_itemsets = set()   # Store discovered frequent itemsets\n",
        "    infrequent_itemsets = set() # Store discovered infrequent itemsets\n",
        "    k = 1\n",
        "\n",
        "    # Initialize frequent 1-itemsets\n",
        "    candidate_1_itemsets = {frozenset([item]) for transaction in transactions.values() for item in transaction}\n",
        "    current_frequent_itemsets = {itemset for itemset in candidate_1_itemsets if calculate_support(transactions, itemset) >= min_count}\n",
        "\n",
        "    # Bidirectional search\n",
        "    while current_frequent_itemsets:\n",
        "        # Add current frequent itemsets to the global set of frequent itemsets\n",
        "        frequent_itemsets.update(current_frequent_itemsets)\n",
        "\n",
        "        # Generate candidates of size k+1\n",
        "        k += 1\n",
        "        candidate_itemsets = generate_candidates(current_frequent_itemsets, k)\n",
        "\n",
        "        # Filter candidates to find frequent and infrequent itemsets\n",
        "        current_frequent_itemsets = set()\n",
        "        for itemset in candidate_itemsets:\n",
        "            support = calculate_support(transactions, itemset)\n",
        "            if support >= min_count:\n",
        "                current_frequent_itemsets.add(itemset)\n",
        "            else:\n",
        "                infrequent_itemsets.add(itemset)\n",
        "\n",
        "        # Prune candidates with infrequent supersets (top-down pruning)\n",
        "        maximal_frequent = {itemset for itemset in current_frequent_itemsets\n",
        "                            if not any(superset.issubset(itemset) for superset in infrequent_itemsets)}\n",
        "\n",
        "        # Add maximal frequent patterns to results\n",
        "        max_frequent_patterns.extend(maximal_frequent)\n",
        "\n",
        "    return max_frequent_patterns\n",
        "\n",
        "# Run Pincer-Search algorithm\n",
        "maximal_frequent_patterns = pincer_search(transactions, min_count)\n",
        "\n",
        "# Output the results\n",
        "print(\"Maximal Frequent Patterns with min_support =\", min_support)\n",
        "for pattern in maximal_frequent_patterns:\n",
        "    print(set(pattern))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "tqngaWlofgMC",
        "outputId": "623f3ffc-2d3f-474a-fa1b-a61cdbe18626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximal Frequent Patterns with min_support = 0.1\n",
            "{'I5', 'I4'}\n",
            "{'I3', 'I1'}\n",
            "{'I3', 'I4'}\n",
            "{'I2', 'I4'}\n",
            "{'I2', 'I1'}\n",
            "{'I1', 'I4'}\n",
            "{'I3', 'I5'}\n",
            "{'I2', 'I5'}\n",
            "{'I1', 'I5'}\n",
            "{'I3', 'I2'}\n",
            "{'I3', 'I1', 'I4'}\n",
            "{'I2', 'I1', 'I4'}\n",
            "{'I1', 'I5', 'I4'}\n",
            "{'I3', 'I2', 'I4'}\n",
            "{'I3', 'I2', 'I1'}\n",
            "{'I2', 'I1', 'I5'}\n",
            "{'I3', 'I1', 'I5'}\n",
            "{'I3', 'I5', 'I4'}\n",
            "{'I2', 'I5', 'I4'}\n",
            "{'I3', 'I2', 'I5'}\n",
            "{'I3', 'I2', 'I5', 'I4'}\n",
            "{'I3', 'I1', 'I2', 'I5'}\n",
            "{'I1', 'I2', 'I5', 'I4'}\n",
            "{'I3', 'I2', 'I1', 'I4'}\n",
            "{'I3', 'I1', 'I5', 'I4'}\n",
            "{'I2', 'I1', 'I4', 'I3', 'I5'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Pincer Search on mushroom dataset - maximal frequent patterns\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Load the mushroom dataset\n",
        "def load_data(filename):\n",
        "    dataset = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            transaction = frozenset(map(int, line.strip().split()))\n",
        "            dataset.append(transaction)\n",
        "    return dataset\n",
        "\n",
        "# Calculate support for each itemset\n",
        "def calculate_support(dataset, itemset):\n",
        "    return sum(1 for transaction in dataset if itemset.issubset(transaction))\n",
        "\n",
        "# Generate candidate itemsets by extending frequent itemsets\n",
        "def generate_candidates(frequent_itemsets, k):\n",
        "    candidates = set()\n",
        "    frequent_itemsets = list(frequent_itemsets)\n",
        "    for i in range(len(frequent_itemsets)):\n",
        "        for j in range(i + 1, len(frequent_itemsets)):\n",
        "            candidate = frequent_itemsets[i] | frequent_itemsets[j]\n",
        "            if len(candidate) == k:\n",
        "                candidates.add(candidate)\n",
        "    return candidates\n",
        "\n",
        "# Perform the Pincer-Search algorithm to find maximal patterns\n",
        "def pincer_search_maximal(dataset, min_support_fraction):\n",
        "    num_transactions = len(dataset)\n",
        "    min_support_count = int(num_transactions * min_support_fraction)\n",
        "\n",
        "    # Initial frequent 1-itemsets\n",
        "    item_counts = defaultdict(int)\n",
        "    for transaction in dataset:\n",
        "        for item in transaction:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {itemset for itemset, count in item_counts.items() if count >= min_support_count}\n",
        "    maximal_itemsets = set()\n",
        "\n",
        "    # Main Pincer-Search loop\n",
        "    k = 2\n",
        "    while frequent_itemsets:\n",
        "        # Generate candidate itemsets from frequent itemsets\n",
        "        candidates = generate_candidates(frequent_itemsets, k)\n",
        "\n",
        "        # Calculate support and prune non-frequent itemsets\n",
        "        frequent_itemsets = set()\n",
        "        for candidate in candidates:\n",
        "            support = calculate_support(dataset, candidate)\n",
        "            if support >= min_support_count:\n",
        "                frequent_itemsets.add(candidate)\n",
        "\n",
        "                # Check for maximal property\n",
        "                is_maximal = True\n",
        "                for superset in maximal_itemsets:\n",
        "                    if candidate < superset:\n",
        "                        is_maximal = False\n",
        "                        break\n",
        "                if is_maximal:\n",
        "                    maximal_itemsets.add(candidate)\n",
        "\n",
        "        # Remove subsets of newly found frequent itemsets from maximal itemsets\n",
        "        for itemset in list(maximal_itemsets):\n",
        "            if any(itemset < frequent for frequent in frequent_itemsets):\n",
        "                maximal_itemsets.discard(itemset)\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    # Return maximal frequent itemsets with their support counts\n",
        "    maximal_itemsets_with_support = {itemset: calculate_support(dataset, itemset) for itemset in maximal_itemsets}\n",
        "    return maximal_itemsets_with_support\n",
        "\n",
        "# Set parameters\n",
        "filename = '/content/drive/MyDrive/mushroom_dataset/mushroom.dat'\n",
        "min_support_fraction = 0.6\n",
        "\n",
        "# Run the algorithm and display results\n",
        "dataset = load_data(filename)\n",
        "maximal_patterns = pincer_search_maximal(dataset, min_support_fraction)\n",
        "print(\"Maximal Frequent Patterns and their Supports:\")\n",
        "for pattern, support in maximal_patterns.items():\n",
        "    print(f\"Pattern: {set(pattern)}, Support: {support}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "xNqc2Ndfqbbl",
        "outputId": "8cb6a499-7711-47ad-ed6f-9dffe0e143ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximal Frequent Patterns and their Supports:\n",
            "Pattern: {34, 59, 85, 86}, Support: 4984\n",
            "Pattern: {85, 63}, Support: 4936\n",
            "Pattern: {34, 85, 86, 39}, Support: 5402\n",
            "Pattern: {90, 85, 39}, Support: 4976\n",
            "Pattern: {34, 36, 85, 86, 90}, Support: 6272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Decision tree model - class example\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Sample dataset based on the provided table\n",
        "data = {\n",
        "    'age': ['<=30', '<=30', '31…40', '>40', '>40', '>40', '31…40', '<=30', '<=30', '>40', '<=30', '31…40', '31…40', '>40'],\n",
        "    'income': ['high', 'high', 'high', 'medium', 'low', 'low', 'low', 'medium', 'low', 'medium', 'medium', 'medium', 'high', 'medium'],\n",
        "    'student': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no'],\n",
        "    'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'excellent'],\n",
        "    'buys_computer': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n",
        "}\n",
        "\n",
        "# Convert data into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to predict based on the decision tree logic\n",
        "def decision_tree(row):\n",
        "    # Level 1: Age\n",
        "    if row['age'] == '<=30':\n",
        "        # Check if student\n",
        "        if row['student'] == 'yes':\n",
        "            return 'yes'\n",
        "        else:\n",
        "            return 'no'\n",
        "    elif row['age'] == '31…40':\n",
        "        return 'yes'\n",
        "    elif row['age'] == '>40':\n",
        "        # Check credit rating\n",
        "        if row['credit_rating'] == 'excellent':\n",
        "            return 'no'\n",
        "        else:  # credit_rating == 'fair'\n",
        "            return 'yes'\n",
        "    return None\n",
        "\n",
        "# Apply the decision tree function to each row\n",
        "df['prediction'] = df.apply(decision_tree, axis=1)\n",
        "\n",
        "# Print the DataFrame with predictions\n",
        "print(df[['age', 'student', 'credit_rating', 'buys_computer', 'prediction']])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (df['buys_computer'] == df['prediction']).mean() * 100\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(df['buys_computer'], df['prediction'], labels=['yes', 'no'])\n",
        "conf_df = pd.DataFrame(conf_matrix, index=['Actual Yes', 'Actual No'], columns=['Predicted Yes', 'Predicted No'])\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_df)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(df['buys_computer'], df['prediction'], target_names=['No', 'Yes']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "K2sKmAzB69Sz",
        "outputId": "54c6282c-d76a-49a6-ff27-e67712023966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      age student credit_rating buys_computer prediction\n",
            "0    <=30      no          fair            no         no\n",
            "1    <=30      no     excellent            no         no\n",
            "2   31…40      no          fair           yes        yes\n",
            "3     >40      no          fair           yes        yes\n",
            "4     >40     yes          fair           yes        yes\n",
            "5     >40     yes     excellent            no         no\n",
            "6   31…40     yes     excellent           yes        yes\n",
            "7    <=30      no          fair            no         no\n",
            "8    <=30     yes          fair           yes        yes\n",
            "9     >40     yes          fair           yes        yes\n",
            "10   <=30     yes     excellent           yes        yes\n",
            "11  31…40      no     excellent           yes        yes\n",
            "12  31…40     yes          fair           yes        yes\n",
            "13    >40      no     excellent            no         no\n",
            "\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Confusion Matrix:\n",
            "            Predicted Yes  Predicted No\n",
            "Actual Yes              9             0\n",
            "Actual No               0             5\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       1.00      1.00      1.00         5\n",
            "         Yes       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        14\n",
            "   macro avg       1.00      1.00      1.00        14\n",
            "weighted avg       1.00      1.00      1.00        14\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Decision tree model - breast cancer cell classification\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy*100)\n",
        "\n",
        "# Classification report and confusion matrix\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "u-kkn74DV_ZS",
        "outputId": "77b3f2f5-2858-483e-9078-a1ba849d0411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.15204678362574\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92        63\n",
            "           1       0.97      0.94      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.94      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 60   3]\n",
            " [  7 101]]\n"
          ]
        }
      ]
    }
  ]
}