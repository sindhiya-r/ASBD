{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9433621,
          "sourceType": "datasetVersion",
          "datasetId": 5731631
        }
      ],
      "dockerImageVersionId": 30775,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ApvOop-sdZd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # COM 526 P ANALYTICS & SYSTEMS OF BIG DATA PRACTICE â€“ PROBLEM SET V"
      ],
      "metadata": {
        "id": "QfxaVXP9dZeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import Binarizer, LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "avocado_df = pd.read_csv('/kaggle/input/dataset/Avocado Dataset.csv')\n",
        "trail_df = pd.read_csv('/kaggle/input/dataset/Trail.csv - Trail.csv.csv')\n",
        "\n",
        "# 1. Select a subset of relevant attributes from Avocado Dataset (PLU 4046, 4225, 4770)\n",
        "filtered_avocado_df = avocado_df[\n",
        "    (avocado_df['4046'] > 0) | (avocado_df['4225'] > 0) | (avocado_df['4770'] > 0) & (avocado_df['type'] == 'organic')\n",
        "]\n",
        "relevant_columns = ['Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'type', 'year', 'region']\n",
        "filtered_avocado_relevant_df = filtered_avocado_df[relevant_columns]\n",
        "\n",
        "# Display the first few rows of the filtered Avocado dataset\n",
        "print(\"Filtered Avocado Data (PLU 4046, 4225, 4770, Organic):\")\n",
        "print(filtered_avocado_relevant_df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:10:50.164519Z",
          "iopub.execute_input": "2024-09-24T06:10:50.165131Z",
          "iopub.status.idle": "2024-09-24T06:10:50.253092Z",
          "shell.execute_reply.started": "2024-09-24T06:10:50.165079Z",
          "shell.execute_reply": "2024-09-24T06:10:50.251204Z"
        },
        "trusted": true,
        "id": "5UU8Nn2CdZeE",
        "outputId": "8b5d99b3-b46a-4ffa-e086-721de30d9dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Filtered Avocado Data (PLU 4046, 4225, 4770, Organic):\n         Date AveragePrice  Total Volume     4046       4225    4770  \\\n0  27-12-2015         1.33      64236.62  1036.74   54454.85   48.16   \n1  20-12-2015         1.35      54876.98   674.28   44638.81   58.33   \n2  13-12-2015         0.93     118220.22   794.70  109149.67  130.50   \n3  06-12-2015         1.08      78992.15  1132.00   71976.41   72.58   \n4  29-11-2015         1.29      51039.60   941.48   43838.39   75.78   \n\n           type  year  region  \n0  conventional  2015  Albany  \n1  conventional  2015  Albany  \n2  conventional  2015  Albany  \n3  conventional  2015  Albany  \n4  conventional  2015  Albany  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove duplicates and fill missing values in \"AveragePrice\" in the Trail Dataset\n",
        "trail_initial_size = trail_df.shape[0]\n",
        "trail_df_cleaned = trail_df.drop_duplicates().copy()\n",
        "trail_df_cleaned['AveragePrice'] = trail_df_cleaned['AveragePrice'].fillna(1.25)  # Avoiding inplace operation\n",
        "trail_final_size = trail_df_cleaned.shape[0]\n",
        "\n",
        "# Print size before and after removing duplicates\n",
        "print(f\"\\nInitial Trail Dataset size: {trail_initial_size}\")\n",
        "print(f\"Final Trail Dataset size (after removing duplicates): {trail_final_size}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:10:59.361709Z",
          "iopub.execute_input": "2024-09-24T06:10:59.362757Z",
          "iopub.status.idle": "2024-09-24T06:10:59.377384Z",
          "shell.execute_reply.started": "2024-09-24T06:10:59.362697Z",
          "shell.execute_reply": "2024-09-24T06:10:59.375605Z"
        },
        "trusted": true,
        "id": "W2ffVjNSdZeI",
        "outputId": "39ae71b0-6c61-41a2-fe34-8f00f9003d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nInitial Trail Dataset size: 202\nFinal Trail Dataset size (after removing duplicates): 195\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Binarize the \"Year\" column in the Avocado Dataset (Threshold: 2016)\n",
        "binarizer = Binarizer(threshold=2016)\n",
        "avocado_df['Year_bin'] = binarizer.fit_transform(avocado_df[['year']])\n",
        "\n",
        "# Display binarized Year column\n",
        "print(\"\\nBinarized Year Column (Threshold: 2016):\")\n",
        "print(avocado_df[['year', 'Year_bin']].head())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:10:36.930100Z",
          "iopub.execute_input": "2024-09-24T06:10:36.931223Z",
          "iopub.status.idle": "2024-09-24T06:10:36.949084Z",
          "shell.execute_reply.started": "2024-09-24T06:10:36.931167Z",
          "shell.execute_reply": "2024-09-24T06:10:36.947769Z"
        },
        "trusted": true,
        "id": "j0pU3kbFdZeJ",
        "outputId": "baff6d15-62fb-4fb2-b1ee-43366e2cb16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nBinarized Year Column (Threshold: 2016):\n   year  Year_bin\n0  2015         0\n1  2015         0\n2  2015         0\n3  2015         0\n4  2015         0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Integer Encoding for categorical attributes in the Avocado Dataset\n",
        "label_encoder = LabelEncoder()\n",
        "avocado_df['type_encoded'] = label_encoder.fit_transform(avocado_df['type'])\n",
        "\n",
        "# Display encoded Type column\n",
        "print(\"\\nInteger Encoded 'Type' Column:\")\n",
        "print(avocado_df[['type', 'type_encoded']].head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:10:09.953022Z",
          "iopub.execute_input": "2024-09-24T06:10:09.953567Z",
          "iopub.status.idle": "2024-09-24T06:10:09.975049Z",
          "shell.execute_reply.started": "2024-09-24T06:10:09.953521Z",
          "shell.execute_reply": "2024-09-24T06:10:09.972984Z"
        },
        "trusted": true,
        "id": "wDAUUacldZeK",
        "outputId": "8896461b-9e60-4d9a-f6b8-62435cb26705"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nInteger Encoded 'Type' Column:\n           type  type_encoded\n0  conventional             0\n1  conventional             0\n2  conventional             0\n3  conventional             0\n4  conventional             0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. One-Hot Encoding for the \"Region\" column in the Avocado Dataset\n",
        "region_encoded = pd.get_dummies(avocado_df['region'], prefix='Region')\n",
        "avocado_df = pd.concat([avocado_df, region_encoded], axis=1)\n",
        "\n",
        "# Display one-hot encoded columns for Region\n",
        "print(\"\\nOne-Hot Encoded 'Region' Columns:\")\n",
        "print(avocado_df.filter(like='Region_').head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:09:46.632675Z",
          "iopub.execute_input": "2024-09-24T06:09:46.633823Z",
          "iopub.status.idle": "2024-09-24T06:09:46.665776Z",
          "shell.execute_reply.started": "2024-09-24T06:09:46.633767Z",
          "shell.execute_reply": "2024-09-24T06:09:46.664484Z"
        },
        "trusted": true,
        "id": "_T4ZevfVdZeL",
        "outputId": "1b22ce6e-ae5a-41e3-8fec-457dc0cbee14"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nOne-Hot Encoded 'Region' Columns:\n   Region_Albany  Region_Atlanta  Region_BaltimoreWashington  Region_Boise  \\\n0           True           False                       False         False   \n1           True           False                       False         False   \n2           True           False                       False         False   \n3           True           False                       False         False   \n4           True           False                       False         False   \n\n   Region_Boston  Region_BuffaloRochester  Region_California  \\\n0          False                    False              False   \n1          False                    False              False   \n2          False                    False              False   \n3          False                    False              False   \n4          False                    False              False   \n\n   Region_Charlotte  Region_Chicago  Region_CincinnatiDayton  ...  \\\n0             False           False                    False  ...   \n1             False           False                    False  ...   \n2             False           False                    False  ...   \n3             False           False                    False  ...   \n4             False           False                    False  ...   \n\n   Region_SouthCarolina  Region_SouthCentral  Region_Southeast  \\\n0                 False                False             False   \n1                 False                False             False   \n2                 False                False             False   \n3                 False                False             False   \n4                 False                False             False   \n\n   Region_Spokane  Region_StLouis  Region_Syracuse  Region_Tampa  \\\n0           False           False            False         False   \n1           False           False            False         False   \n2           False           False            False         False   \n3           False           False            False         False   \n4           False           False            False         False   \n\n   Region_TotalUS  Region_West  Region_WestTexNewMexico  \n0           False        False                    False  \n1           False        False                    False  \n2           False        False                    False  \n3           False        False                    False  \n4           False        False                    False  \n\n[5 rows x 108 columns]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Ignore tuples with missing values in the Avocado Dataset\n",
        "avocado_cleaned_df = avocado_df.dropna()\n",
        "\n",
        "# Display cleaned Avocado dataset (without NaN values)\n",
        "print(\"\\nAvocado Data after removing rows with missing values:\")\n",
        "print(avocado_cleaned_df.head())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:09:24.951883Z",
          "iopub.execute_input": "2024-09-24T06:09:24.952947Z",
          "iopub.status.idle": "2024-09-24T06:09:24.990584Z",
          "shell.execute_reply.started": "2024-09-24T06:09:24.952850Z",
          "shell.execute_reply": "2024-09-24T06:09:24.989286Z"
        },
        "trusted": true,
        "id": "B5pwZitudZeN",
        "outputId": "7603734f-e3ae-4b28-9de1-9087f4e5893b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nAvocado Data after removing rows with missing values:\n         Date AveragePrice  Total Volume     4046       4225    4770  \\\n0  27-12-2015         1.33      64236.62  1036.74   54454.85   48.16   \n1  20-12-2015         1.35      54876.98   674.28   44638.81   58.33   \n2  13-12-2015         0.93     118220.22   794.70  109149.67  130.50   \n3  06-12-2015         1.08      78992.15  1132.00   71976.41   72.58   \n4  29-11-2015         1.29      51039.60   941.48   43838.39   75.78   \n\n   Total Bags  Small Bags  Large Bags  XLarge Bags  ... Region_SouthCarolina  \\\n0     8696.87     8603.62       93.25          0.0  ...                False   \n1     9505.56     9408.07       97.49          0.0  ...                False   \n2     8145.35     8042.21      103.14          0.0  ...                False   \n3     5811.16     5677.40      133.76          0.0  ...                False   \n4     6183.95     5986.26      197.69          0.0  ...                False   \n\n   Region_SouthCentral Region_Southeast  Region_Spokane  Region_StLouis  \\\n0                False            False           False           False   \n1                False            False           False           False   \n2                False            False           False           False   \n3                False            False           False           False   \n4                False            False           False           False   \n\n   Region_Syracuse  Region_Tampa  Region_TotalUS  Region_West  \\\n0            False         False           False        False   \n1            False         False           False        False   \n2            False         False           False        False   \n3            False         False           False        False   \n4            False         False           False        False   \n\n   Region_WestTexNewMexico  \n0                    False  \n1                    False  \n2                    False  \n3                    False  \n4                    False  \n\n[5 rows x 69 columns]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop the attribute with high nullity in the Avocado Dataset\n",
        "nullity_threshold = avocado_df.isnull().mean().idxmax()\n",
        "avocado_df_dropped = avocado_df.drop(columns=[nullity_threshold])\n",
        "\n",
        "print(f\"\\nDropped column with highest nullity: {nullity_threshold}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:08:06.499208Z",
          "iopub.execute_input": "2024-09-24T06:08:06.499706Z",
          "iopub.status.idle": "2024-09-24T06:08:06.523268Z",
          "shell.execute_reply.started": "2024-09-24T06:08:06.499662Z",
          "shell.execute_reply": "2024-09-24T06:08:06.521398Z"
        },
        "trusted": true,
        "id": "SkA2mhlXdZeN",
        "outputId": "ad12c134-a8e5-430c-8cc4-b140b8e9275a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nDropped column with highest nullity: AveragePrice\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Statistical Summary of the Avocado Dataset (for numeric columns only)\n",
        "numeric_columns = avocado_df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
        "stat_summary = {\n",
        "    'Dimensions': avocado_df.shape,\n",
        "    'Most Frequent Values': avocado_df.mode().iloc[0],\n",
        "    'Data Types': avocado_df.dtypes,\n",
        "    'Count': avocado_df.count(),\n",
        "    'Mean': numeric_columns.mean(),\n",
        "    'Standard Deviation': numeric_columns.std(),\n",
        "    'Min': numeric_columns.min(),\n",
        "    'Max': numeric_columns.max(),\n",
        "    '25%': numeric_columns.quantile(0.25),\n",
        "    'Median (50%)': numeric_columns.median(),\n",
        "    '75%': numeric_columns.quantile(0.75),\n",
        "    'Class Distribution (Type)': avocado_df['type'].value_counts(normalize=True),\n",
        "    'Correlation Matrix': numeric_columns.corr(),\n",
        "    'Skewness': numeric_columns.skew(),\n",
        "}\n",
        "\n",
        "# Display statistical summary\n",
        "print(\"\\nStatistical Summary of the Avocado Dataset:\")\n",
        "print(stat_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T06:07:41.020197Z",
          "iopub.execute_input": "2024-09-24T06:07:41.020743Z",
          "iopub.status.idle": "2024-09-24T06:07:41.161782Z",
          "shell.execute_reply.started": "2024-09-24T06:07:41.020695Z",
          "shell.execute_reply": "2024-09-24T06:07:41.160406Z"
        },
        "trusted": true,
        "id": "rWsTjxMvdZeP",
        "outputId": "e08824d3-54c8-4ba3-b578-36c960a1a291"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nStatistical Summary of the Avocado Dataset:\n{'Dimensions': (18250, 69), 'Most Frequent Values': Date                       18-03-2018\nAveragePrice                     1.15\nTotal Volume                  2038.99\n4046                              0.0\n4225                              0.0\n                              ...    \nRegion_Syracuse                 False\nRegion_Tampa                    False\nRegion_TotalUS                  False\nRegion_West                     False\nRegion_WestTexNewMexico         False\nName: 0, Length: 69, dtype: object, 'Data Types': Date                        object\nAveragePrice                object\nTotal Volume               float64\n4046                       float64\n4225                       float64\n                            ...   \nRegion_Syracuse               bool\nRegion_Tampa                  bool\nRegion_TotalUS                bool\nRegion_West                   bool\nRegion_WestTexNewMexico       bool\nLength: 69, dtype: object, 'Count': Date                       18250\nAveragePrice               18222\nTotal Volume               18250\n4046                       18250\n4225                       18250\n                           ...  \nRegion_Syracuse            18250\nRegion_Tampa               18250\nRegion_TotalUS             18250\nRegion_West                18250\nRegion_WestTexNewMexico    18250\nLength: 69, dtype: int64, 'Mean': Total Volume    850598.273413\n4046            292992.481896\n4225            295138.477670\n4770             22838.484500\nTotal Bags      239626.747390\nSmall Bags      182185.367250\nLarge Bags       54335.123135\nXLarge Bags       3106.256292\nyear              2016.148000\nYear_bin             0.384603\ntype_encoded         0.499945\ndtype: float64, 'Standard Deviation': Total Volume    3.453456e+06\n4046            1.264956e+06\n4225            1.204089e+06\n4770            1.074613e+05\nTotal Bags      9.862168e+05\nSmall Bags      7.461591e+05\nLarge Bags      2.439596e+05\nXLarge Bags     1.769242e+04\nyear            9.400127e-01\nYear_bin        4.865146e-01\ntype_encoded    5.000137e-01\ndtype: float64, 'Min': Total Volume      84.56\n4046               0.00\n4225               0.00\n4770               0.00\nTotal Bags         0.00\nSmall Bags         0.00\nLarge Bags         0.00\nXLarge Bags        0.00\nyear            2015.00\nYear_bin           0.00\ntype_encoded       0.00\ndtype: float64, 'Max': Total Volume    62505646.52\n4046            22743616.17\n4225            20470572.61\n4770             2546439.11\nTotal Bags      19373134.37\nSmall Bags      13384586.80\nLarge Bags       5719096.61\nXLarge Bags       551693.65\nyear                2018.00\nYear_bin               1.00\ntype_encoded           1.00\ndtype: float64, '25%': Total Volume    10839.6275\n4046              854.2100\n4225             3008.0975\n4770                0.0000\nTotal Bags       5089.0825\nSmall Bags       2850.3225\nLarge Bags        127.5800\nXLarge Bags         0.0000\nyear             2015.0000\nYear_bin            0.0000\ntype_encoded        0.0000\nName: 0.25, dtype: float64, 'Median (50%)': Total Volume    107365.505\n4046              8643.200\n4225             29058.875\n4770               184.975\nTotal Bags       39741.180\nSmall Bags       26351.615\nLarge Bags        2647.270\nXLarge Bags          0.000\nyear              2016.000\nYear_bin             0.000\ntype_encoded         0.000\ndtype: float64, '75%': Total Volume    432952.6650\n4046            111008.7125\n4225            150166.3350\n4770              6242.0550\nTotal Bags      110781.1150\nSmall Bags       83336.2100\nLarge Bags       22018.2750\nXLarge Bags        132.4325\nyear              2017.0000\nYear_bin             1.0000\ntype_encoded         1.0000\nName: 0.75, dtype: float64, 'Class Distribution (Type)': type\nconventional    0.500055\norganic         0.499945\nName: proportion, dtype: float64, 'Correlation Matrix':               Total Volume      4046      4225      4770  Total Bags  \\\nTotal Volume      1.000000  0.977863  0.974181  0.872203    0.963047   \n4046              0.977863  1.000000  0.926110  0.833390    0.920057   \n4225              0.974181  0.926110  1.000000  0.887855    0.905788   \n4770              0.872203  0.833390  0.887855  1.000000    0.792315   \nTotal Bags        0.963047  0.920057  0.905788  0.792315    1.000000   \nSmall Bags        0.967238  0.925280  0.916031  0.802733    0.994335   \nLarge Bags        0.880640  0.838645  0.810016  0.698472    0.943009   \nXLarge Bags       0.747158  0.699378  0.688809  0.679862    0.804233   \nyear              0.017165  0.003328 -0.009584 -0.036550    0.071520   \nYear_bin          0.011308  0.004989 -0.011139 -0.048374    0.052067   \ntype_encoded     -0.232441 -0.225825 -0.232296 -0.210033   -0.217795   \n\n              Small Bags  Large Bags  XLarge Bags      year  Year_bin  \\\nTotal Volume    0.967238    0.880640     0.747158  0.017165  0.011308   \n4046            0.925280    0.838645     0.699378  0.003328  0.004989   \n4225            0.916031    0.810016     0.688809 -0.009584 -0.011139   \n4770            0.802733    0.698472     0.679862 -0.036550 -0.048374   \nTotal Bags      0.994335    0.943009     0.804233  0.071520  0.052067   \nSmall Bags      1.000000    0.902589     0.806845  0.063883  0.046032   \nLarge Bags      0.902589    1.000000     0.710859  0.087858  0.065349   \nXLarge Bags     0.806845    0.710859     1.000000  0.081005  0.059873   \nyear            0.063883    0.087858     0.081005  1.000000  0.871957   \nYear_bin        0.046032    0.065349     0.059873  0.871957  1.000000   \ntype_encoded   -0.220541   -0.193184    -0.175488  0.000076 -0.000026   \n\n              type_encoded  \nTotal Volume     -0.232441  \n4046             -0.225825  \n4225             -0.232296  \n4770             -0.210033  \nTotal Bags       -0.217795  \nSmall Bags       -0.220541  \nLarge Bags       -0.193184  \nXLarge Bags      -0.175488  \nyear              0.000076  \nYear_bin         -0.000026  \ntype_encoded      1.000000  , 'Skewness': Total Volume     9.007930\n4046             8.648456\n4225             8.942706\n4770            10.159671\nTotal Bags       9.756334\nSmall Bags       9.540917\nLarge Bags       9.796719\nXLarge Bags     13.140106\nyear             0.215371\nYear_bin         0.474436\ntype_encoded     0.000219\ndtype: float64}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Test drive the use of Gini Index, Information Gain, Entropy and other measures that are supported in your platform, performing the role of data selection.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Feature1': [1, 1, 0, 0, 1, 0, 1, 0],\n",
        "    'Feature2': [1, 0, 1, 0, 1, 1, 0, 0],\n",
        "    'Label':    [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to calculate entropy\n",
        "def entropy(probabilities):\n",
        "    return -sum([p * log2(p) for p in probabilities if p != 0])\n",
        "\n",
        "# Function to calculate Gini Index\n",
        "def gini(probabilities):\n",
        "    return 1 - sum([p**2 for p in probabilities])\n",
        "\n",
        "# Function to calculate Information Gain\n",
        "def information_gain(df, feature, label):\n",
        "    # Calculate total entropy for the dataset\n",
        "    total_entropy = entropy(df[label].value_counts(normalize=True))\n",
        "\n",
        "    # Split the data by the feature and calculate entropy for each subset\n",
        "    values = df[feature].unique()\n",
        "    weighted_entropy = 0\n",
        "    for value in values:\n",
        "        subset = df[df[feature] == value]\n",
        "        subset_entropy = entropy(subset[label].value_counts(normalize=True))\n",
        "        weighted_entropy += len(subset) / len(df) * subset_entropy\n",
        "\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# Apply calculations for each feature\n",
        "features = ['Feature1', 'Feature2']\n",
        "results = {}\n",
        "\n",
        "for feature in features:\n",
        "    info_gain = information_gain(df, feature, 'Label')\n",
        "    gini_index = gini(df[feature].value_counts(normalize=True))\n",
        "    results[feature] = {'Information Gain': info_gain, 'Gini Index': gini_index}\n",
        "\n",
        "# Display the results\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T17:08:16.134146Z",
          "iopub.execute_input": "2024-09-24T17:08:16.134543Z",
          "iopub.status.idle": "2024-09-24T17:08:17.242960Z",
          "shell.execute_reply.started": "2024-09-24T17:08:16.134502Z",
          "shell.execute_reply": "2024-09-24T17:08:17.241687Z"
        },
        "trusted": true,
        "id": "FWF4TXP_dZeQ",
        "outputId": "39fee657-6a33-4d0f-de00-88bc9b2d29e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          Information Gain  Gini Index\nFeature1          0.188722         0.5\nFeature2          0.188722         0.5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Information Gain</th>\n      <th>Gini Index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Feature1</th>\n      <td>0.188722</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>Feature2</th>\n      <td>0.188722</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Test drive the implementation support in your platform of choice for data preprocessing phases such as cleaning, selection, transformation, integration in addition to the earlier exercises.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# --- Sample dataset with missing values and duplicates ---\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Alice'],\n",
        "    'Age': [25, np.nan, 35, 45, 25],\n",
        "    'Salary': [50000, 60000, np.nan, 80000, 50000],\n",
        "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Step 1: Data Cleaning ---\n",
        "# Remove duplicates\n",
        "df_cleaned = df.drop_duplicates()\n",
        "\n",
        "# Fill missing values using .loc[] to avoid SettingWithCopyWarning\n",
        "df_cleaned.loc[:, 'Age'] = df_cleaned['Age'].fillna(df_cleaned['Age'].mean())\n",
        "df_cleaned.loc[:, 'Salary'] = df_cleaned['Salary'].fillna(df_cleaned['Salary'].median())\n",
        "\n",
        "# --- Step 2: Feature Selection ---\n",
        "# For demonstration, we'll use only numerical features ('Age' and 'Salary') for selection\n",
        "X = df_cleaned[['Age', 'Salary']]\n",
        "y = np.random.randint(0, 2, len(df_cleaned))  # Example target variable for feature selection\n",
        "\n",
        "# Select the best features (k=1 for simplicity here)\n",
        "selector = SelectKBest(score_func=f_classif, k=1)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[selector.get_support(indices=True)]\n",
        "\n",
        "# --- Step 3: Data Transformation ---\n",
        "# Scale the selected numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "# One-hot encode the categorical column using updated `sparse_output` parameter\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Updated argument to sparse_output\n",
        "X_encoded = encoder.fit_transform(df_cleaned[['Gender']])\n",
        "\n",
        "# Combine scaled and encoded features into a final dataset\n",
        "X_preprocessed = np.concatenate([X_scaled, X_encoded], axis=1)\n",
        "\n",
        "# --- Step 4: Data Integration (Optional, if you have multiple datasets) ---\n",
        "# Example: Integrating another dataset (Assume we have an additional dataset)\n",
        "additional_data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'City': ['NYC', 'LA', 'SF', 'NYC']\n",
        "}\n",
        "\n",
        "df_additional = pd.DataFrame(additional_data)\n",
        "\n",
        "# Merge the cleaned dataset with the additional dataset on 'Name'\n",
        "df_merged = pd.merge(df_cleaned, df_additional, on='Name')\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"Cleaned DataFrame:\")\n",
        "print(df_cleaned)\n",
        "print(\"\\nSelected Feature(s):\", selected_features)\n",
        "print(\"\\nPreprocessed Features (Scaled + Encoded):\")\n",
        "print(X_preprocessed)\n",
        "print(\"\\nMerged DataFrame:\")\n",
        "print(df_merged)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T17:24:47.689469Z",
          "iopub.execute_input": "2024-09-24T17:24:47.690087Z",
          "iopub.status.idle": "2024-09-24T17:24:47.720365Z",
          "shell.execute_reply.started": "2024-09-24T17:24:47.690044Z",
          "shell.execute_reply": "2024-09-24T17:24:47.719311Z"
        },
        "trusted": true,
        "id": "H_GEjgNndZeR",
        "outputId": "f2c89212-a945-40aa-a796-84d6fddd54a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cleaned DataFrame:\n      Name   Age   Salary  Gender\n0    Alice  25.0  50000.0  Female\n1      Bob  35.0  60000.0    Male\n2  Charlie  35.0  60000.0    Male\n3    David  45.0  80000.0    Male\n\nSelected Feature(s): Index(['Salary'], dtype='object')\n\nPreprocessed Features (Scaled + Encoded):\n[[-1.14707867  1.          0.        ]\n [-0.22941573  0.          1.        ]\n [-0.22941573  0.          1.        ]\n [ 1.60591014  0.          1.        ]]\n\nMerged DataFrame:\n      Name   Age   Salary  Gender City\n0    Alice  25.0  50000.0  Female  NYC\n1      Bob  35.0  60000.0    Male   LA\n2  Charlie  35.0  60000.0    Male   SF\n3    David  45.0  80000.0    Male  NYC\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}