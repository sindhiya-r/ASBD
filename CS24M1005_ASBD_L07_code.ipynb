{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOF7ij94gfdO"
      },
      "source": [
        "Lab Statement 7\n",
        "\n",
        "(1) Extend the Apriori Algorithm discussed in the class supporting Transaction Reduction\n",
        "approach to improve the time complexity issue as a result of the repeated scans limitation of\n",
        "Apriori. You may compare this extended version with the earlier implementations in FP\n",
        "Growth over the same benchmark dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaReU1FNgfdP",
        "outputId": "c7879426-74b8-42b4-e6bf-b14798a66b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUNNING TIME OF ALGORITHMS FOR TRANSACTIONS OF mushroom.dat FIMI DATASET\n",
            "\n",
            "Apriori Time: 1.0472 seconds\n",
            "FP-Growth Time: 0.0578 seconds\n",
            "\n",
            "It is noted that FP-Growth takes less time than Apriori for finding frequent itemset in same transactions\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Function to load the mushroom dataset\n",
        "def load_mushroom_data(file_path):\n",
        "    transactions = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into items (space-separated)\n",
        "            transaction = line.strip().split()\n",
        "            transactions.append(transaction)\n",
        "    return transactions\n",
        "\n",
        "# Helper function to calculate support\n",
        "def calculate_support(itemset, transactions):\n",
        "    return sum(1 for transaction in transactions if set(itemset).issubset(set(transaction)))\n",
        "\n",
        "# Apriori Algorithm with Transaction Reduction\n",
        "def apriori_transaction_reduction(transactions, min_support):\n",
        "    items = {item for transaction in transactions for item in transaction}\n",
        "    candidates = [{item} for item in items]\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    while candidates:\n",
        "        candidate_support = {frozenset(candidate): calculate_support(candidate, transactions) for candidate in candidates}\n",
        "        frequent_candidates = {itemset: support for itemset, support in candidate_support.items() if support >= min_support}\n",
        "\n",
        "        if not frequent_candidates:\n",
        "            break\n",
        "\n",
        "        transactions = [[item for item in transaction if {item}.issubset(set(frequent_candidates))] for transaction in transactions]\n",
        "        frequent_itemsets.extend(frequent_candidates)\n",
        "\n",
        "        candidate_items = set().union(*frequent_candidates)\n",
        "        next_candidate_size = len(list(frequent_candidates)[0]) + 1\n",
        "        candidates = [set(comb) for comb in combinations(candidate_items, next_candidate_size) if len(set(comb)) == next_candidate_size]\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# FP-Growth Algorithm\n",
        "class TreeNode:\n",
        "    def __init__(self, item, count=0):\n",
        "        self.item = item\n",
        "        self.count = count\n",
        "        self.children = {}\n",
        "        self.parent = None\n",
        "        self.link = None\n",
        "\n",
        "def insert_tree(items, node, header_table):\n",
        "    first_item = items[0]\n",
        "    if first_item in node.children:\n",
        "        node.children[first_item].count += 1\n",
        "    else:\n",
        "        new_node = TreeNode(first_item, 1)\n",
        "        new_node.parent = node\n",
        "        node.children[first_item] = new_node\n",
        "\n",
        "        if first_item in header_table:\n",
        "            last_node = header_table[first_item][1]\n",
        "            while last_node.link is not None:\n",
        "                last_node = last_node.link\n",
        "            last_node.link = new_node\n",
        "        else:\n",
        "            header_table[first_item] = (1, new_node)\n",
        "\n",
        "    if len(items) > 1:\n",
        "        insert_tree(items[1:], node.children[first_item], header_table)\n",
        "\n",
        "def fp_growth(transactions, min_support):\n",
        "    item_freq = defaultdict(int)\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_freq[item] += 1\n",
        "\n",
        "    item_freq = {k: v for k, v in item_freq.items() if v >= min_support}\n",
        "\n",
        "    root = TreeNode(None)\n",
        "    header_table = {}\n",
        "\n",
        "    for transaction in transactions:\n",
        "        sorted_items = [item for item in sorted(transaction, key=lambda i: item_freq.get(i, 0), reverse=True) if item in item_freq]\n",
        "        if sorted_items:\n",
        "            insert_tree(sorted_items, root, header_table)\n",
        "\n",
        "    return item_freq\n",
        "\n",
        "# Function to compare running times\n",
        "def compare_algorithms(transactions, min_support_ratio):\n",
        "    total_transactions = len(transactions)\n",
        "    min_support = int(min_support_ratio * total_transactions)\n",
        "\n",
        "    apriori_start_time = time.time()\n",
        "    apriori_result = apriori_transaction_reduction(transactions, min_support)\n",
        "    apriori_time = time.time() - apriori_start_time\n",
        "\n",
        "    fp_growth_start_time = time.time()\n",
        "    fp_growth_result = fp_growth(transactions, min_support)\n",
        "    fp_growth_time = time.time() - fp_growth_start_time\n",
        "\n",
        "    return apriori_time, fp_growth_time\n",
        "\n",
        "# Sample execution using the mushroom.dat dataset\n",
        "file_path = 'mushroom.dat'  # Path to mushroom.dat\n",
        "transactions = load_mushroom_data(file_path)\n",
        "min_support_ratio = 0.6  # Set the minimum support ratio (30%)\n",
        "\n",
        "# Compare running times\n",
        "apriori_time, fp_growth_time = compare_algorithms(transactions, min_support_ratio)\n",
        "\n",
        "# Print results only once\n",
        "print(\"RUNNING TIME OF ALGORITHMS FOR TRANSACTIONS OF mushroom.dat FIMI DATASET\\n\")\n",
        "print(f\"Apriori Time: {apriori_time:.4f} seconds\")\n",
        "print(f\"FP-Growth Time: {fp_growth_time:.4f} seconds\\n\")\n",
        "print(\"It is noted that FP-Growth takes less time than Apriori for finding frequent itemset in same transactions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3lI4VsigfdR"
      },
      "source": [
        "(2) Test drive any one implementation in FP growth or Transaction Reduction adopting a Vertical Transaction Database format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWRDoNYBgfdR",
        "outputId": "bfcb8e1a-47e9-4a16-c4b7-cdd221dcd1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequent Itemsets using Vertical Transaction Database (VTD) format with min support count:\n",
            "\n",
            "Itemset: {'1'}, Total occurence: 3916\n",
            "Itemset: {'3'}, Total occurence: 3656\n",
            "Itemset: {'9'}, Total occurence: 2556\n",
            "Itemset: {'13'}, Total occurence: 2284\n",
            "Itemset: {'23'}, Total occurence: 3376\n",
            "Itemset: {'34'}, Total occurence: 7914\n",
            "Itemset: {'36'}, Total occurence: 6812\n",
            "Itemset: {'38'}, Total occurence: 2512\n",
            "Itemset: {'52'}, Total occurence: 3516\n",
            "Itemset: {'54'}, Total occurence: 1120\n",
            "Itemset: {'59'}, Total occurence: 5176\n",
            "Itemset: {'63'}, Total occurence: 4936\n",
            "Itemset: {'67'}, Total occurence: 4464\n",
            "Itemset: {'76'}, Total occurence: 4384\n",
            "Itemset: {'85'}, Total occurence: 8124\n",
            "Itemset: {'86'}, Total occurence: 7924\n",
            "Itemset: {'90'}, Total occurence: 7488\n",
            "Itemset: {'93'}, Total occurence: 3968\n",
            "Itemset: {'98'}, Total occurence: 1872\n",
            "Itemset: {'107'}, Total occurence: 1248\n",
            "Itemset: {'2'}, Total occurence: 4208\n",
            "Itemset: {'14'}, Total occurence: 1072\n",
            "Itemset: {'39'}, Total occurence: 5612\n",
            "Itemset: {'99'}, Total occurence: 1968\n",
            "Itemset: {'114'}, Total occurence: 2148\n",
            "Itemset: {'15'}, Total occurence: 1040\n",
            "Itemset: {'41'}, Total occurence: 1048\n",
            "Itemset: {'10'}, Total occurence: 3244\n",
            "Itemset: {'16'}, Total occurence: 1840\n",
            "Itemset: {'24'}, Total occurence: 4748\n",
            "Itemset: {'28'}, Total occurence: 3528\n",
            "Itemset: {'37'}, Total occurence: 1312\n",
            "Itemset: {'53'}, Total occurence: 4608\n",
            "Itemset: {'94'}, Total occurence: 2776\n",
            "Itemset: {'43'}, Total occurence: 1492\n",
            "Itemset: {'110'}, Total occurence: 4040\n",
            "Itemset: {'44'}, Total occurence: 1202\n",
            "Itemset: {'11'}, Total occurence: 2320\n",
            "Itemset: {'111'}, Total occurence: 1712\n",
            "Itemset: {'6'}, Total occurence: 3152\n",
            "Itemset: {'56'}, Total occurence: 3776\n",
            "Itemset: {'116'}, Total occurence: 3148\n",
            "Itemset: {'117'}, Total occurence: 1144\n",
            "Itemset: {'77'}, Total occurence: 1872\n",
            "Itemset: {'69'}, Total occurence: 1872\n",
            "Itemset: {'17'}, Total occurence: 1500\n",
            "Itemset: {'29'}, Total occurence: 2160\n",
            "Itemset: {'61'}, Total occurence: 2372\n",
            "Itemset: {'66'}, Total occurence: 2304\n",
            "Itemset: {'95'}, Total occurence: 1296\n",
            "Itemset: {'101'}, Total occurence: 1632\n",
            "Itemset: {'58'}, Total occurence: 2480\n",
            "Itemset: {'102'}, Total occurence: 2388\n",
            "Itemset: {'48'}, Total occurence: 1728\n",
            "Itemset: {'119'}, Total occurence: 832\n",
            "Itemset: {'7'}, Total occurence: 828\n",
            "\n",
            "Total Number of Frequent Itemsets: 56\n",
            "Execution Time: 0.0629 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# Function to load the mushroom dataset\n",
        "def load_mushroom_data(file_path):\n",
        "    transactions = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into items (space-separated)\n",
        "            transaction = line.strip().split()\n",
        "            transactions.append(transaction)\n",
        "    return transactions\n",
        "\n",
        "# Convert horizontal transactions to vertical transaction database (VTD)\n",
        "def convert_to_vertical_db(transactions):\n",
        "    vertical_db = defaultdict(set)\n",
        "    for tid, transaction in enumerate(transactions):\n",
        "        for item in transaction:\n",
        "            vertical_db[item].add(tid)\n",
        "    return vertical_db\n",
        "\n",
        "# Helper function to get the intersection of sets\n",
        "def get_intersection(itemset, vertical_db):\n",
        "    # Check if all items in the itemset are in the vertical_db\n",
        "    if all(item in vertical_db for item in itemset):\n",
        "        item_tids = [vertical_db[item] for item in itemset]\n",
        "        return set.intersection(*item_tids) if item_tids else set()\n",
        "    else:\n",
        "        return set()\n",
        "\n",
        "# Transaction Reduction using Vertical Transaction Database format\n",
        "def apriori_vtd(transactions, min_support_count):\n",
        "    vertical_db = convert_to_vertical_db(transactions)\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    # Initialize single itemsets (1-itemsets)\n",
        "    candidates = [{item} for item in vertical_db]\n",
        "\n",
        "    while candidates:\n",
        "        # Calculate support and form frequent candidates\n",
        "        candidate_support = {frozenset(candidate): len(get_intersection(candidate, vertical_db)) for candidate in candidates}\n",
        "        frequent_candidates = {itemset: support for itemset, support in candidate_support.items() if support >= min_support_count}\n",
        "\n",
        "        if not frequent_candidates:\n",
        "            break\n",
        "\n",
        "        # Reduce vertical_db based on frequent candidates\n",
        "        reduced_db = {item: tids for item, tids in vertical_db.items() if {item}.issubset(set(frequent_candidates))}\n",
        "        vertical_db = reduced_db\n",
        "        frequent_itemsets.extend(frequent_candidates.items())  # Store as tuples (itemset, support)\n",
        "\n",
        "        # Generate next candidate size (k-itemsets)\n",
        "        candidate_items = set().union(*frequent_candidates)\n",
        "        next_candidate_size = len(list(frequent_candidates)[0]) + 1\n",
        "        candidates = [set(comb) for comb in combinations(candidate_items, next_candidate_size) if len(set(comb)) == next_candidate_size]\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Function to calculate minimum support count\n",
        "def calculate_min_support_count(transactions, min_support_ratio):\n",
        "    total_transactions = len(transactions)\n",
        "    return int(min_support_ratio * total_transactions)\n",
        "\n",
        "# Sample execution using the mushroom.dat dataset\n",
        "file_path = 'mushroom.dat'  # Path to mushroom.dat\n",
        "transactions = load_mushroom_data(file_path)\n",
        "min_support_ratio = 0.1 # Set the minimum support ratio\n",
        "\n",
        "# Calculate minimum support count\n",
        "min_support_count = calculate_min_support_count(transactions, min_support_ratio)\n",
        "\n",
        "# Run the Apriori algorithm using VTD format\n",
        "start_time = time.time()\n",
        "frequent_itemsets_vtd = apriori_vtd(transactions, min_support_count)\n",
        "end_time = time.time()\n",
        "\n",
        "# Print results\n",
        "print(\"Frequent Itemsets using Vertical Transaction Database (VTD) format with min support count:\\n\")\n",
        "\n",
        "# Counter for number of frequent itemsets\n",
        "num_frequent_itemsets = 0\n",
        "\n",
        "for itemset, support in frequent_itemsets_vtd:  # Now expect (itemset, support)\n",
        "    print(f\"Itemset: {set(itemset)}, Total occurence: {support}\")\n",
        "    num_frequent_itemsets += 1\n",
        "\n",
        "# Print number of frequent itemsets\n",
        "print(f\"\\nTotal Number of Frequent Itemsets: {num_frequent_itemsets}\")\n",
        "print(f\"Execution Time: {end_time - start_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzEWcfZjgfdS"
      },
      "source": [
        "(3) Using a vertical transaction database notation, generate the FI’s following the intersection\n",
        "approach (basic ECLAT) discussed in the class. Use benchmark datasets in FIMI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAy2vdbogfdS",
        "outputId": "b9307d91-b0a2-4735-e890-35d4b1899db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequent Itemsets (count: 255):\n",
            "1. ['34']: 7914\n",
            "2. ['34', '36']: 6812\n",
            "3. ['34', '36', '59']: 5176\n",
            "4. ['34', '36', '59', '63']: 4936\n",
            "5. ['34', '36', '59', '63', '85']: 8124\n",
            "6. ['34', '36', '59', '63', '85', '86']: 7924\n",
            "7. ['34', '36', '59', '63', '85', '86', '90']: 7488\n",
            "8. ['34', '36', '39', '59', '63', '85', '86', '90']: 5612\n",
            "9. ['34', '36', '39', '59', '63', '85', '86']: 5612\n",
            "10. ['34', '36', '59', '63', '85', '90']: 7488\n",
            "11. ['34', '36', '39', '59', '63', '85', '90']: 5612\n",
            "12. ['34', '36', '39', '59', '63', '85']: 5612\n",
            "13. ['34', '36', '59', '63', '86']: 7924\n",
            "14. ['34', '36', '59', '63', '86', '90']: 7488\n",
            "15. ['34', '36', '39', '59', '63', '86', '90']: 5612\n",
            "16. ['34', '36', '39', '59', '63', '86']: 5612\n",
            "17. ['34', '36', '59', '63', '90']: 7488\n",
            "18. ['34', '36', '39', '59', '63', '90']: 5612\n",
            "19. ['34', '36', '39', '59', '63']: 5612\n",
            "20. ['34', '36', '59', '85']: 8124\n",
            "21. ['34', '36', '59', '85', '86']: 7924\n",
            "22. ['34', '36', '59', '85', '86', '90']: 7488\n",
            "23. ['34', '36', '39', '59', '85', '86', '90']: 5612\n",
            "24. ['34', '36', '39', '59', '85', '86']: 5612\n",
            "25. ['34', '36', '59', '85', '90']: 7488\n",
            "26. ['34', '36', '39', '59', '85', '90']: 5612\n",
            "27. ['34', '36', '39', '59', '85']: 5612\n",
            "28. ['34', '36', '59', '86']: 7924\n",
            "29. ['34', '36', '59', '86', '90']: 7488\n",
            "30. ['34', '36', '39', '59', '86', '90']: 5612\n",
            "31. ['34', '36', '39', '59', '86']: 5612\n",
            "32. ['34', '36', '59', '90']: 7488\n",
            "33. ['34', '36', '39', '59', '90']: 5612\n",
            "34. ['34', '36', '39', '59']: 5612\n",
            "35. ['34', '36', '63']: 4936\n",
            "36. ['34', '36', '63', '85']: 8124\n",
            "37. ['34', '36', '63', '85', '86']: 7924\n",
            "38. ['34', '36', '63', '85', '86', '90']: 7488\n",
            "39. ['34', '36', '39', '63', '85', '86', '90']: 5612\n",
            "40. ['34', '36', '39', '63', '85', '86']: 5612\n",
            "41. ['34', '36', '63', '85', '90']: 7488\n",
            "42. ['34', '36', '39', '63', '85', '90']: 5612\n",
            "43. ['34', '36', '39', '63', '85']: 5612\n",
            "44. ['34', '36', '63', '86']: 7924\n",
            "45. ['34', '36', '63', '86', '90']: 7488\n",
            "46. ['34', '36', '39', '63', '86', '90']: 5612\n",
            "47. ['34', '36', '39', '63', '86']: 5612\n",
            "48. ['34', '36', '63', '90']: 7488\n",
            "49. ['34', '36', '39', '63', '90']: 5612\n",
            "50. ['34', '36', '39', '63']: 5612\n",
            "51. ['34', '36', '85']: 8124\n",
            "52. ['34', '36', '85', '86']: 7924\n",
            "53. ['34', '36', '85', '86', '90']: 7488\n",
            "54. ['34', '36', '39', '85', '86', '90']: 5612\n",
            "55. ['34', '36', '39', '85', '86']: 5612\n",
            "56. ['34', '36', '85', '90']: 7488\n",
            "57. ['34', '36', '39', '85', '90']: 5612\n",
            "58. ['34', '36', '39', '85']: 5612\n",
            "59. ['34', '36', '86']: 7924\n",
            "60. ['34', '36', '86', '90']: 7488\n",
            "61. ['34', '36', '39', '86', '90']: 5612\n",
            "62. ['34', '36', '39', '86']: 5612\n",
            "63. ['34', '36', '90']: 7488\n",
            "64. ['34', '36', '39', '90']: 5612\n",
            "65. ['34', '36', '39']: 5612\n",
            "66. ['34', '59']: 5176\n",
            "67. ['34', '59', '63']: 4936\n",
            "68. ['34', '59', '63', '85']: 8124\n",
            "69. ['34', '59', '63', '85', '86']: 7924\n",
            "70. ['34', '59', '63', '85', '86', '90']: 7488\n",
            "71. ['34', '39', '59', '63', '85', '86', '90']: 5612\n",
            "72. ['34', '39', '59', '63', '85', '86']: 5612\n",
            "73. ['34', '59', '63', '85', '90']: 7488\n",
            "74. ['34', '39', '59', '63', '85', '90']: 5612\n",
            "75. ['34', '39', '59', '63', '85']: 5612\n",
            "76. ['34', '59', '63', '86']: 7924\n",
            "77. ['34', '59', '63', '86', '90']: 7488\n",
            "78. ['34', '39', '59', '63', '86', '90']: 5612\n",
            "79. ['34', '39', '59', '63', '86']: 5612\n",
            "80. ['34', '59', '63', '90']: 7488\n",
            "81. ['34', '39', '59', '63', '90']: 5612\n",
            "82. ['34', '39', '59', '63']: 5612\n",
            "83. ['34', '59', '85']: 8124\n",
            "84. ['34', '59', '85', '86']: 7924\n",
            "85. ['34', '59', '85', '86', '90']: 7488\n",
            "86. ['34', '39', '59', '85', '86', '90']: 5612\n",
            "87. ['34', '39', '59', '85', '86']: 5612\n",
            "88. ['34', '59', '85', '90']: 7488\n",
            "89. ['34', '39', '59', '85', '90']: 5612\n",
            "90. ['34', '39', '59', '85']: 5612\n",
            "91. ['34', '59', '86']: 7924\n",
            "92. ['34', '59', '86', '90']: 7488\n",
            "93. ['34', '39', '59', '86', '90']: 5612\n",
            "94. ['34', '39', '59', '86']: 5612\n",
            "95. ['34', '59', '90']: 7488\n",
            "96. ['34', '39', '59', '90']: 5612\n",
            "97. ['34', '39', '59']: 5612\n",
            "98. ['34', '63']: 4936\n",
            "99. ['34', '63', '85']: 8124\n",
            "100. ['34', '63', '85', '86']: 7924\n",
            "101. ['34', '63', '85', '86', '90']: 7488\n",
            "102. ['34', '39', '63', '85', '86', '90']: 5612\n",
            "103. ['34', '39', '63', '85', '86']: 5612\n",
            "104. ['34', '63', '85', '90']: 7488\n",
            "105. ['34', '39', '63', '85', '90']: 5612\n",
            "106. ['34', '39', '63', '85']: 5612\n",
            "107. ['34', '63', '86']: 7924\n",
            "108. ['34', '63', '86', '90']: 7488\n",
            "109. ['34', '39', '63', '86', '90']: 5612\n",
            "110. ['34', '39', '63', '86']: 5612\n",
            "111. ['34', '63', '90']: 7488\n",
            "112. ['34', '39', '63', '90']: 5612\n",
            "113. ['34', '39', '63']: 5612\n",
            "114. ['34', '85']: 8124\n",
            "115. ['34', '85', '86']: 7924\n",
            "116. ['34', '85', '86', '90']: 7488\n",
            "117. ['34', '39', '85', '86', '90']: 5612\n",
            "118. ['34', '39', '85', '86']: 5612\n",
            "119. ['34', '85', '90']: 7488\n",
            "120. ['34', '39', '85', '90']: 5612\n",
            "121. ['34', '39', '85']: 5612\n",
            "122. ['34', '86']: 7924\n",
            "123. ['34', '86', '90']: 7488\n",
            "124. ['34', '39', '86', '90']: 5612\n",
            "125. ['34', '39', '86']: 5612\n",
            "126. ['34', '90']: 7488\n",
            "127. ['34', '39', '90']: 5612\n",
            "128. ['34', '39']: 5612\n",
            "129. ['36']: 6812\n",
            "130. ['36', '59']: 5176\n",
            "131. ['36', '59', '63']: 4936\n",
            "132. ['36', '59', '63', '85']: 8124\n",
            "133. ['36', '59', '63', '85', '86']: 7924\n",
            "134. ['36', '59', '63', '85', '86', '90']: 7488\n",
            "135. ['36', '39', '59', '63', '85', '86', '90']: 5612\n",
            "136. ['36', '39', '59', '63', '85', '86']: 5612\n",
            "137. ['36', '59', '63', '85', '90']: 7488\n",
            "138. ['36', '39', '59', '63', '85', '90']: 5612\n",
            "139. ['36', '39', '59', '63', '85']: 5612\n",
            "140. ['36', '59', '63', '86']: 7924\n",
            "141. ['36', '59', '63', '86', '90']: 7488\n",
            "142. ['36', '39', '59', '63', '86', '90']: 5612\n",
            "143. ['36', '39', '59', '63', '86']: 5612\n",
            "144. ['36', '59', '63', '90']: 7488\n",
            "145. ['36', '39', '59', '63', '90']: 5612\n",
            "146. ['36', '39', '59', '63']: 5612\n",
            "147. ['36', '59', '85']: 8124\n",
            "148. ['36', '59', '85', '86']: 7924\n",
            "149. ['36', '59', '85', '86', '90']: 7488\n",
            "150. ['36', '39', '59', '85', '86', '90']: 5612\n",
            "151. ['36', '39', '59', '85', '86']: 5612\n",
            "152. ['36', '59', '85', '90']: 7488\n",
            "153. ['36', '39', '59', '85', '90']: 5612\n",
            "154. ['36', '39', '59', '85']: 5612\n",
            "155. ['36', '59', '86']: 7924\n",
            "156. ['36', '59', '86', '90']: 7488\n",
            "157. ['36', '39', '59', '86', '90']: 5612\n",
            "158. ['36', '39', '59', '86']: 5612\n",
            "159. ['36', '59', '90']: 7488\n",
            "160. ['36', '39', '59', '90']: 5612\n",
            "161. ['36', '39', '59']: 5612\n",
            "162. ['36', '63']: 4936\n",
            "163. ['36', '63', '85']: 8124\n",
            "164. ['36', '63', '85', '86']: 7924\n",
            "165. ['36', '63', '85', '86', '90']: 7488\n",
            "166. ['36', '39', '63', '85', '86', '90']: 5612\n",
            "167. ['36', '39', '63', '85', '86']: 5612\n",
            "168. ['36', '63', '85', '90']: 7488\n",
            "169. ['36', '39', '63', '85', '90']: 5612\n",
            "170. ['36', '39', '63', '85']: 5612\n",
            "171. ['36', '63', '86']: 7924\n",
            "172. ['36', '63', '86', '90']: 7488\n",
            "173. ['36', '39', '63', '86', '90']: 5612\n",
            "174. ['36', '39', '63', '86']: 5612\n",
            "175. ['36', '63', '90']: 7488\n",
            "176. ['36', '39', '63', '90']: 5612\n",
            "177. ['36', '39', '63']: 5612\n",
            "178. ['36', '85']: 8124\n",
            "179. ['36', '85', '86']: 7924\n",
            "180. ['36', '85', '86', '90']: 7488\n",
            "181. ['36', '39', '85', '86', '90']: 5612\n",
            "182. ['36', '39', '85', '86']: 5612\n",
            "183. ['36', '85', '90']: 7488\n",
            "184. ['36', '39', '85', '90']: 5612\n",
            "185. ['36', '39', '85']: 5612\n",
            "186. ['36', '86']: 7924\n",
            "187. ['36', '86', '90']: 7488\n",
            "188. ['36', '39', '86', '90']: 5612\n",
            "189. ['36', '39', '86']: 5612\n",
            "190. ['36', '90']: 7488\n",
            "191. ['36', '39', '90']: 5612\n",
            "192. ['36', '39']: 5612\n",
            "193. ['59']: 5176\n",
            "194. ['59', '63']: 4936\n",
            "195. ['59', '63', '85']: 8124\n",
            "196. ['59', '63', '85', '86']: 7924\n",
            "197. ['59', '63', '85', '86', '90']: 7488\n",
            "198. ['39', '59', '63', '85', '86', '90']: 5612\n",
            "199. ['39', '59', '63', '85', '86']: 5612\n",
            "200. ['59', '63', '85', '90']: 7488\n",
            "201. ['39', '59', '63', '85', '90']: 5612\n",
            "202. ['39', '59', '63', '85']: 5612\n",
            "203. ['59', '63', '86']: 7924\n",
            "204. ['59', '63', '86', '90']: 7488\n",
            "205. ['39', '59', '63', '86', '90']: 5612\n",
            "206. ['39', '59', '63', '86']: 5612\n",
            "207. ['59', '63', '90']: 7488\n",
            "208. ['39', '59', '63', '90']: 5612\n",
            "209. ['39', '59', '63']: 5612\n",
            "210. ['59', '85']: 8124\n",
            "211. ['59', '85', '86']: 7924\n",
            "212. ['59', '85', '86', '90']: 7488\n",
            "213. ['39', '59', '85', '86', '90']: 5612\n",
            "214. ['39', '59', '85', '86']: 5612\n",
            "215. ['59', '85', '90']: 7488\n",
            "216. ['39', '59', '85', '90']: 5612\n",
            "217. ['39', '59', '85']: 5612\n",
            "218. ['59', '86']: 7924\n",
            "219. ['59', '86', '90']: 7488\n",
            "220. ['39', '59', '86', '90']: 5612\n",
            "221. ['39', '59', '86']: 5612\n",
            "222. ['59', '90']: 7488\n",
            "223. ['39', '59', '90']: 5612\n",
            "224. ['39', '59']: 5612\n",
            "225. ['63']: 4936\n",
            "226. ['63', '85']: 8124\n",
            "227. ['63', '85', '86']: 7924\n",
            "228. ['63', '85', '86', '90']: 7488\n",
            "229. ['39', '63', '85', '86', '90']: 5612\n",
            "230. ['39', '63', '85', '86']: 5612\n",
            "231. ['63', '85', '90']: 7488\n",
            "232. ['39', '63', '85', '90']: 5612\n",
            "233. ['39', '63', '85']: 5612\n",
            "234. ['63', '86']: 7924\n",
            "235. ['63', '86', '90']: 7488\n",
            "236. ['39', '63', '86', '90']: 5612\n",
            "237. ['39', '63', '86']: 5612\n",
            "238. ['63', '90']: 7488\n",
            "239. ['39', '63', '90']: 5612\n",
            "240. ['39', '63']: 5612\n",
            "241. ['85']: 8124\n",
            "242. ['85', '86']: 7924\n",
            "243. ['85', '86', '90']: 7488\n",
            "244. ['39', '85', '86', '90']: 5612\n",
            "245. ['39', '85', '86']: 5612\n",
            "246. ['85', '90']: 7488\n",
            "247. ['39', '85', '90']: 5612\n",
            "248. ['39', '85']: 5612\n",
            "249. ['86']: 7924\n",
            "250. ['86', '90']: 7488\n",
            "251. ['39', '86', '90']: 5612\n",
            "252. ['39', '86']: 5612\n",
            "253. ['90']: 7488\n",
            "254. ['39', '90']: 5612\n",
            "255. ['39']: 5612\n",
            "\n",
            "Execution Time: 0.0050 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_dataset(file_path):\n",
        "    transactions = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            transaction = line.strip().split()\n",
        "            transactions.append(transaction)\n",
        "    return transactions\n",
        "\n",
        "# Function to convert transactions to vertical format\n",
        "def convert_to_vertical(transactions):\n",
        "    vertical_db = defaultdict(list)\n",
        "    for idx, transaction in enumerate(transactions):\n",
        "        for item in transaction:\n",
        "            vertical_db[item].append(idx)  # Append transaction index for each item\n",
        "    return vertical_db\n",
        "\n",
        "# ECLAT implementation using vertical database format\n",
        "def eclat(vertical_db, min_support_count):\n",
        "    # Initialize frequent itemsets\n",
        "    frequent_itemsets = {}\n",
        "\n",
        "    # Convert vertical database into a list of items\n",
        "    items = list(vertical_db.keys())\n",
        "\n",
        "    # Helper function for recursive ECLAT\n",
        "    def eclat_recursive(prefix, items, vertical_db):\n",
        "        for i in range(len(items)):\n",
        "            item = items[i]\n",
        "            new_prefix = prefix + [item]\n",
        "            transactions = vertical_db[item]\n",
        "\n",
        "            # Check the support\n",
        "            if len(transactions) >= min_support_count:\n",
        "                frequent_itemsets[frozenset(new_prefix)] = len(transactions)\n",
        "\n",
        "                # Continue recursively\n",
        "                eclat_recursive(new_prefix, items[i+1:], vertical_db)\n",
        "\n",
        "    # Start the recursive ECLAT\n",
        "    eclat_recursive([], items, vertical_db)\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Function to compare running times and execute ECLAT\n",
        "def compare_eclat(transactions, min_support_ratio):\n",
        "    total_transactions = len(transactions)\n",
        "    min_support_count = int(min_support_ratio * total_transactions)  # Calculate the minimum support count\n",
        "\n",
        "    vertical_db = convert_to_vertical(transactions)\n",
        "\n",
        "    start_time = time.time()\n",
        "    frequent_itemsets = eclat(vertical_db, min_support_count)\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    return frequent_itemsets, execution_time\n",
        "\n",
        "# Sample execution using a benchmark dataset from FIMI\n",
        "file_path = 'mushroom.dat'\n",
        "transactions = load_dataset(file_path)\n",
        "min_support_ratio = 0.6  # Set minimum support as a ratio\n",
        "\n",
        "# Run the ECLAT algorithm\n",
        "frequent_itemsets, execution_time = compare_eclat(transactions, min_support_ratio)\n",
        "\n",
        "# Print results in a structured format\n",
        "print(\"Frequent Itemsets (count: {}):\".format(len(frequent_itemsets)))\n",
        "for i, itemset in enumerate(frequent_itemsets.keys(), start=1):\n",
        "    print(f\"{i}. {sorted(list(itemset))}: {frequent_itemsets[itemset]}\")  # Print itemset and its support\n",
        "\n",
        "print(f\"\\nExecution Time: {execution_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfTLoZqqgfdT"
      },
      "source": [
        "(4) Extend the basic Apriori algorithm to generate Frequent Patterns which differentiate ab\n",
        "from ba (ordered patterns generation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIJxwCT6gfdT",
        "outputId": "c5fa8e38-9d24-4d65-cd28-96499978afff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of frequent items (Apriori): 51\n",
            "Frequent items: {frozenset({'85', '90', '86'}), frozenset({'85', '39', '86'}), frozenset({'34', '85', '39', '86'}), frozenset({'85'}), frozenset({'39'}), frozenset({'36'}), frozenset({'34'}), frozenset({'36', '85', '34', '86'}), frozenset({'59', '85', '86'}), frozenset({'85', '90'}), frozenset({'36', '34', '86'}), frozenset({'36', '90', '86'}), frozenset({'90', '34', '86'}), frozenset({'90', '34'}), frozenset({'59', '34', '86'}), frozenset({'34', '85', '39'}), frozenset({'36', '90'}), frozenset({'36', '34'}), frozenset({'59', '85'}), frozenset({'85', '86'}), frozenset({'39', '86'}), frozenset({'90', '39'}), frozenset({'90', '86'}), frozenset({'36', '85', '34'}), frozenset({'63', '85'}), frozenset({'36', '90', '85'}), frozenset({'90', '34', '85'}), frozenset({'59', '85', '34'}), frozenset({'85', '34', '86'}), frozenset({'34', '39', '86'}), frozenset({'36', '85', '86'}), frozenset({'36', '90', '34'}), frozenset({'59', '86'}), frozenset({'85', '39'}), frozenset({'90', '39', '85'}), frozenset({'36', '90', '34', '85'}), frozenset({'59', '85', '34', '86'}), frozenset({'36', '85'}), frozenset({'34', '39'}), frozenset({'85', '34'}), frozenset({'90'}), frozenset({'36', '90', '85', '86'}), frozenset({'36', '90', '34', '86'}), frozenset({'36', '86'}), frozenset({'59'}), frozenset({'86'}), frozenset({'59', '34'}), frozenset({'34', '86'}), frozenset({'90', '86', '34', '85'}), frozenset({'63'}), frozenset({'90', '34', '86', '36', '85'})}\n",
            "Total number of ordered patterns: 95\n",
            "Ordered patterns: {('85', '39', '86'), ('39', '86'), ('36', '85', '34', '86'), ('36', '90', '34', '86'), ('63', '90'), ('36', '39', '86'), ('59', '39'), ('36', '90', '85'), ('36', '85', '39'), ('59', '85', '34'), ('63', '85', '34'), ('90', '39', '34', '85'), ('59', '85'), ('90', '39', '85'), ('36', '90', '34', '85'), ('59', '85', '34', '86'), ('34', '39', '86'), ('85', '90', '86'), ('63', '39'), ('36', '59', '85', '34'), ('36', '90', '86'), ('59', '85', '39', '86'), ('59', '63'), ('90', '39'), ('36', '90', '39', '85'), ('36', '39', '34'), ('59', '85', '90'), ('63', '85', '90'), ('59', '34', '86'), ('34', '85', '39', '86'), ('90', '39', '34', '86'), ('59', '86'), ('90', '39', '86'), ('59', '90', '34', '85'), ('36', '39'), ('36', '85', '86'), ('36', '59', '34', '86'), ('90', '86', '34', '85'), ('36', '39', '34', '86'), ('59', '36'), ('36', '85', '39', '86'), ('85', '39'), ('36', '90', '34'), ('36', '85'), ('59', '85', '39', '34'), ('59', '39', '34', '86', '85'), ('63', '86'), ('59', '90', '34', '86'), ('39', '34', '86', '36', '85'), ('34', '85', '39'), ('90', '86'), ('59', '34'), ('90', '39', '34'), ('59', '36', '85'), ('63', '36', '85'), ('36', '86'), ('36', '85', '34'), ('63', '85'), ('36', '85', '39', '34'), ('59', '63', '85'), ('59', '34', '86', '36', '85'), ('59', '39', '86'), ('85', '86'), ('59', '36', '86'), ('59', '90'), ('90', '34'), ('36', '90', '85', '86'), ('59', '85', '39'), ('63', '85', '39'), ('85', '34', '86'), ('90', '59', '85', '86'), ('36', '34', '86'), ('34', '39'), ('36', '34'), ('59', '39', '34', '86'), ('63', '36'), ('59', '90', '86'), ('90', '39', '34', '86', '85'), ('59', '39', '34'), ('85', '34'), ('90', '34', '85'), ('59', '36', '34'), ('36', '90'), ('63', '34'), ('59', '90', '34', '86', '85'), ('59', '85', '86'), ('63', '85', '86'), ('90', '85', '39', '86'), ('90', '34', '86'), ('85', '90'), ('36', '90', '39'), ('59', '90', '34'), ('34', '86'), ('36', '59', '85', '86'), ('90', '34', '86', '36', '85')}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_frequent_itemsets_apriori(dataset, min_support):\n",
        "    # Get 1-itemsets\n",
        "    item_count = defaultdict(int)\n",
        "    for transaction in dataset:\n",
        "        for item in transaction:\n",
        "            item_count[item] += 1\n",
        "\n",
        "    frequent_items = {frozenset([item]) for item, count in item_count.items() if count >= min_support}\n",
        "    frequent_itemsets = frequent_items.copy()\n",
        "\n",
        "    # Generate higher-order itemsets with ordered patterns (ab ≠ ba)\n",
        "    k = 2\n",
        "    ordered_patterns = set()\n",
        "    while frequent_items:\n",
        "        candidate_itemsets = {i.union(j) for i in frequent_items for j in frequent_items if len(i.union(j)) == k}\n",
        "        item_count = defaultdict(int)\n",
        "\n",
        "        for transaction in dataset:\n",
        "            for itemset in candidate_itemsets:\n",
        "                if itemset.issubset(transaction):\n",
        "                    item_count[itemset] += 1\n",
        "                    # Ordered pattern generation (ab ≠ ba)\n",
        "                    if tuple(sorted(itemset)) not in ordered_patterns:\n",
        "                        ordered_patterns.add(tuple(itemset))\n",
        "\n",
        "        frequent_items = {itemset for itemset, count in item_count.items() if count >= min_support}\n",
        "        frequent_itemsets.update(frequent_items)\n",
        "        k += 1\n",
        "\n",
        "    return frequent_itemsets, ordered_patterns\n",
        "\n",
        "# Run the Apriori algorithm with ordered patterns\n",
        "apriori_frequent_itemsets, apriori_ordered_patterns = get_frequent_itemsets_apriori(dataset, min_support)\n",
        "\n",
        "print(f\"Total number of frequent items (Apriori): {len(apriori_frequent_itemsets)}\")\n",
        "print(\"Frequent items:\", apriori_frequent_itemsets)\n",
        "print(f\"Total number of ordered patterns: {len(apriori_ordered_patterns)}\")\n",
        "print(\"Ordered patterns:\", apriori_ordered_patterns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggUzj3w-gfdT"
      },
      "source": [
        "(5) Implement the Dynamic Itemset Counting Algorithm for Frequent Itemset Generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6SbrNGrgfdT",
        "outputId": "a4611888-c7b2-4bd3-a97f-c09789ca473e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of frequent items (DIC): 51\n",
            "Frequent items: {frozenset({'85', '90', '86'}), frozenset({'85', '39', '86'}), frozenset({'34', '85', '39', '86'}), frozenset({'85'}), frozenset({'39'}), frozenset({'36'}), frozenset({'34'}), frozenset({'36', '85', '34', '86'}), frozenset({'59', '85', '86'}), frozenset({'85', '90'}), frozenset({'36', '34', '86'}), frozenset({'36', '90', '86'}), frozenset({'90', '34', '86'}), frozenset({'90', '34'}), frozenset({'59', '34', '86'}), frozenset({'85', '39', '34'}), frozenset({'36', '90'}), frozenset({'36', '34'}), frozenset({'59', '85'}), frozenset({'85', '86'}), frozenset({'39', '86'}), frozenset({'90', '39'}), frozenset({'90', '86'}), frozenset({'36', '85', '34'}), frozenset({'63', '85'}), frozenset({'36', '85', '90'}), frozenset({'34', '85', '90'}), frozenset({'59', '85', '34'}), frozenset({'85', '34', '86'}), frozenset({'34', '39', '86'}), frozenset({'36', '85', '86'}), frozenset({'36', '90', '34'}), frozenset({'59', '86'}), frozenset({'85', '39'}), frozenset({'90', '39', '85'}), frozenset({'36', '85', '90', '34'}), frozenset({'59', '85', '34', '86'}), frozenset({'36', '85'}), frozenset({'34', '39'}), frozenset({'85', '34'}), frozenset({'90'}), frozenset({'36', '85', '90', '86'}), frozenset({'36', '90', '34', '86'}), frozenset({'36', '86'}), frozenset({'59'}), frozenset({'86'}), frozenset({'59', '34'}), frozenset({'34', '86'}), frozenset({'34', '85', '90', '86'}), frozenset({'63'}), frozenset({'36', '90', '85', '34', '86'})}\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "# Dynamic Itemset Counting Algorithm (DIC)\n",
        "def dic_algorithm(dataset, min_support):\n",
        "    transactions = len(dataset)\n",
        "    item_count = defaultdict(int)\n",
        "    candidate_itemsets = set()\n",
        "\n",
        "    # Phase 1: Initialize candidate 1-itemsets\n",
        "    for transaction in dataset:\n",
        "        for item in transaction:\n",
        "            item_count[frozenset([item])] += 1\n",
        "            candidate_itemsets.add(frozenset([item]))\n",
        "\n",
        "    frequent_itemsets = {item for item in candidate_itemsets if item_count[item] >= min_support}\n",
        "\n",
        "    # Phase 2: Extend itemsets dynamically\n",
        "    k = 2\n",
        "    while candidate_itemsets:\n",
        "        next_candidates = set()\n",
        "        for itemset1, itemset2 in itertools.combinations(frequent_itemsets, 2):\n",
        "            new_itemset = itemset1 | itemset2\n",
        "            if len(new_itemset) == k:\n",
        "                next_candidates.add(new_itemset)\n",
        "\n",
        "        item_count.update({itemset: 0 for itemset in next_candidates})\n",
        "\n",
        "        for transaction in dataset:\n",
        "            for itemset in next_candidates:\n",
        "                if itemset.issubset(transaction):\n",
        "                    item_count[itemset] += 1\n",
        "\n",
        "        candidate_itemsets = {itemset for itemset in next_candidates if item_count[itemset] >= min_support}\n",
        "        frequent_itemsets.update(candidate_itemsets)\n",
        "        k += 1\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Run Dynamic Itemset Counting Algorithm\n",
        "dic_frequent_patterns = dic_algorithm(dataset, min_support)\n",
        "\n",
        "print(f\"Total number of frequent items (DIC): {len(dic_frequent_patterns)}\")\n",
        "print(\"Frequent items:\", dic_frequent_patterns)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}